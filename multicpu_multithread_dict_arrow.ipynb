{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Trovato: #Rows: 8979525, encoding:UTF-8, chunksize: 1000000, #Chunks:9\n",
      "Chunk 0: Completed ‚úÖ | ElabTime: 25.0 s | 08:53:38\n",
      "Chunk 1: Completed ‚úÖ | ElabTime: 22.0 s | 08:54:00\n",
      "Chunk 2: Completed ‚úÖ | ElabTime: 24.0 s | 08:54:25\n",
      "Chunk 3: Completed ‚úÖ | ElabTime: 23.0 s | 08:54:48\n",
      "Chunk 4: Completed ‚úÖ | ElabTime: 26.0 s | 08:55:14\n",
      "Chunk 5: Completed ‚úÖ | ElabTime: 27.0 s | 08:55:41\n",
      "Chunk 6: Completed ‚úÖ | ElabTime: 29.0 s | 08:56:10\n",
      "Chunk 7: Completed ‚úÖ | ElabTime: 28.0 s | 08:56:38\n",
      "Chunk 8: Completed ‚úÖ | ElabTime: 28.0 s | 08:57:06\n"
     ]
    }
   ],
   "source": [
    "#versione read_sas7bdat\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pyreadstat\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import polars\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "sas_file = \"45cols.sas7bdat\"\n",
    "run_ref = \"_202510_\"\n",
    "extension = \".parquet\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_prefix = sas_file.replace(\".sas7bdat\",\"\")\n",
    "table_folder = file_prefix\n",
    "output_folder = os.path.join(\"output_folder_snappy\", table_folder)\n",
    "\n",
    "\n",
    "chunksize = 1000000\n",
    "offset = 0\n",
    "chunk_idx = 0\n",
    "max_workers = 1\n",
    "\n",
    "#Common Functions\n",
    "def snapshot(label):\n",
    "   current, peak = tracemalloc.get_traced_memory()\n",
    "   print(f\"[{label}] Current: {current / 1024**2:.2f} MB | Peak: {peak / 1024**2:.2f} MB | {timenow()}\")\n",
    "\n",
    "def mem():\n",
    "    mem = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "    return mem\n",
    "\n",
    "def timenow():\n",
    "    timenow = datetime.now().strftime('%H:%M:%S')\n",
    "    return timenow\n",
    "\n",
    "\n",
    "\n",
    "#Operator\n",
    "def process_chunk(output_folder, run_ref, parquet_file, extension, chunk_idx, chunksize):\n",
    "\n",
    "    filepath = os.path.join(output_folder, parquet_file + run_ref + str(chunk_idx) + extension)\n",
    "\n",
    "    chunk_start = time.time()\n",
    "    #print(f\"Chunk {chunk_idx}: Processing üîÑ | {timenow()}\")\n",
    "\n",
    "    #snapshot(\"Start_Chunk\")\n",
    "    offset = chunk_idx * chunksize\n",
    "    \n",
    "    \"\"\"\"\n",
    "     #Ha senso solo se ti carichi tanti GB in una volta\n",
    "    chunk, meta = pyreadstat.read_file_multiprocessing(\n",
    "        pyreadstat.read_sas7bdat,\n",
    "        sas_file,\n",
    "        num_processes = 4,\n",
    "        row_offset=offset,\n",
    "        row_limit=chunksize,\n",
    "        output_format = \"polars\",\n",
    "        encoding = \"windows-1252\",\n",
    "        disable_datetime_conversion = True\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        chunk, meta = pyreadstat.read_sas7bdat(\n",
    "            sas_file,\n",
    "            row_offset=offset,\n",
    "            row_limit=chunksize,\n",
    "            output_format = \"dict\",\n",
    "            disable_datetime_conversion = \"True\",\n",
    "            encoding = \"windows-1252\",\n",
    "            usecols = [\n",
    "                \"libname\", \"memname\", \"memtype\", \"dbms_memtype\",\n",
    "                \"memlabel\",\n",
    "                \"typemem\", \"crdate\", \"modate\", \"nobs\", \"obslen\",\n",
    "                \"nvar\", \"protect\", \"compress\", \"encrypt\", \"npage\",\n",
    "                \"filesize\", \"pcompress\", \"reuse\", \"bufsize\", \"delobs\",\n",
    "                \"nlobs\", \"maxvar\", \"maxlabel\", \"maxgen\", \"gen\", \"attr\",\n",
    "                \"indxtype\", \"datarep\", \"sortname\", \"sorttype\", \"sortchar\",\n",
    "                \"datarepname\", \"encoding\", \"audit\", \"audit_before\", \"audit_admin\",\n",
    "                \"audit_error\", \"audit_data\", \"num_character\", \"num_numeric\"\n",
    "                ]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"{chunk_idx}: Riga{chunk_idx + 1}: Errore critico in lettura: {e}\")\n",
    "    \n",
    "    #snapshot(\"Polars_Done\")\n",
    "    #print(f\"Dopo lettura - RAM usata: {mem():.2f} MB | {timenow()}\")\n",
    "\n",
    "    #if chunk.shape[0] == 0:\n",
    "    # print(f\"{chunk_idx}: Vuoto, Termino Processo | StartingRow: {offset}\")\n",
    "    # sys.exit(1)\n",
    "\n",
    "    table = pa.table(chunk)\n",
    "    del chunk\n",
    "    #snapshot(\"Arrow_Done\")\n",
    "\n",
    "    #print(f\"Dopo arrow - RAM usata: {mem():.2f} MB | {timenow()}\")\n",
    "\n",
    "    #chunk.write_parquet(filepath)\n",
    "    pq.write_table(table, filepath, compression = 'snappy')\n",
    "    #snapshot(\"Parquet_Done\")\n",
    "\n",
    "    del table\n",
    "    \n",
    "    #print(f\"Parquet scritto - RAM usata: {mem():.2f} MB | {timenow()}\")\n",
    "    chunk_end = time.time()\n",
    "    chunk_elab_time = round((chunk_end - chunk_start),0)\n",
    "\n",
    "    print(f\"Chunk {chunk_idx}: Completed ‚úÖ | ElabTime: {chunk_elab_time} s | {timenow()}\")\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "#Main with Iteration\n",
    "def main():\n",
    "\n",
    "    #tracemalloc.start()\n",
    "    if os.path.exists(output_folder):\n",
    "            shutil.rmtree(output_folder)\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "    #Controllo_Iniziale_File\n",
    "\n",
    "    if os.path.exists(sas_file):\n",
    "        _,meta = pyreadstat.read_sas7bdat(sas_file, metadataonly = True)\n",
    "    else:\n",
    "        print(f\"File_Check: File {sas_file} non trovato\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    rows = meta.number_rows\n",
    "    num_chunks = (meta.number_rows + chunksize - 1)//chunksize\n",
    "\n",
    "    print(f\"File Trovato: #Rows: {meta.number_rows}, encoding:{meta.file_encoding}, chunksize: {chunksize}, #Chunks:{num_chunks}\") \n",
    "\n",
    "    with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    process_chunk,\n",
    "                    output_folder,\n",
    "                    run_ref, \n",
    "                    file_prefix,\n",
    "                    extension, \n",
    "                    chunk_idx,\n",
    "                    chunksize\n",
    "                    ): chunk_idx for chunk_idx in range(num_chunks)\n",
    "            }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        idx = futures[future]\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"[Chunk {idx}] Producer error: {e}\")\n",
    "\n",
    "main()\n",
    "\n",
    "#duckdb.sql(\"\"\"\n",
    "#    COPY (SELECT * FROM 'output_folder/*.parquet') \n",
    "#    TO 'merged.parquet' \n",
    "#    (FORMAT PARQUET, CODEC 'snappy');\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Schema del dataset:\n",
      "\n",
      "üìä Numero di righe: 8560000\n",
      "üì¶ Numero di colonne: 15\n",
      "\n",
      "‚è±Ô∏è Tempo di lettura: 0.861 secondi\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "input_folder = \"output_folder_zstd\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Leggi tutti i file Parquet nella cartella come dataset\n",
    "dataset = ds.dataset(input_folder, format=\"parquet\")\n",
    "\n",
    "# Carica l'intero dataset in memoria (solo per test)\n",
    "table = dataset.to_table()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "# Stampa informazioni\n",
    "print(\"üìÑ Schema del dataset:\")\n",
    "print(f\"\\nüìä Numero di righe: {table.num_rows}\")\n",
    "print(f\"üì¶ Numero di colonne: {table.num_columns}\")\n",
    "print(\"\\n‚è±Ô∏è Tempo di lettura: {:.3f} secondi\".format(elapsed))\n",
    "\n",
    "del table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
