{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-1\n",
      "A0\n",
      "A1\n",
      "File Trovato: #Rows: 8979525, chunksize: 1000000, #Chunks:9\n",
      "Chunk Processing: Start\n",
      "Chunk 0: Completed ‚úÖ | ElabTime: 24.0 s | 00:38:46\n",
      "Chunk 1: Completed ‚úÖ | ElabTime: 24.0 s | 00:39:10\n",
      "Chunk 2: Completed ‚úÖ | ElabTime: 25.0 s | 00:39:35\n",
      "Chunk 3: Completed ‚úÖ | ElabTime: 25.0 s | 00:40:00\n",
      "Chunk 4: Completed ‚úÖ | ElabTime: 28.0 s | 00:40:28\n",
      "Chunk 5: Completed ‚úÖ | ElabTime: 28.0 s | 00:40:55\n",
      "Chunk 6: Completed ‚úÖ | ElabTime: 35.0 s | 00:41:30\n",
      "Chunk 7: Completed ‚úÖ | ElabTime: 29.0 s | 00:41:59\n",
      "Chunk 8: Completed ‚úÖ | ElabTime: 29.0 s | 00:42:27\n",
      "Consolidazione Parquet Finale\n",
      "Consolidation: Terminated | ElabTime: 23.0\n"
     ]
    }
   ],
   "source": [
    "#Include:\n",
    "# Gestione in chunks\n",
    "# Gestione esplicita colonne da ingestionare\n",
    "# Gestione esplictita tipi colonne + Validazione dello schema\n",
    "# Consolidamento Parquet\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pyreadstat\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "colspecs = [['BANCA' , '$5.'],['DT_RIFERIMENTO' , 'DATE9.'],['NDG' , '$16.'],['SPORTELLO' , '6'],['TIPO_RAP' , '$2.'],['NUM_CONTO' , '16'],['PROGR_FIDO' , '4'],['FT_FIDO' , '$5.'],['CREDIT_LINE' , '$1.'],['FLAG_RIPARTO' , '$1.'],['PRIORITA' , '4'],['FIDO_1' , '20,2'],['SALDO_CONT_1' , '20,2'],['SCONF_1' , '20,2'],['MARGINE_1' , '20,2'],['SALDO_DISP_1' , '20,2'],['UTILIZZO_1' , '20,2'],['FIDO_2' , '20,2'],['SALDO_CONT_2' , '20,2'],['SCONF_2' , '20,2'],['MARGINE_2' , '20,2'],['SALDO_DISP_2' , '20,2'],['UTILIZZO_2' , '20,2'],['FIDO_3' , '20,2'],['SALDO_CONT_3' , '20,2'],['SCONF_3' , '20,2'],['MARGINE_3' , '20,2'],['SALDO_DISP_3' , '20,2'],['UTILIZZO_3' , '20,2'],['FIDO_4' , '20,2'],['SALDO_CONT_4' , '20,2'],['SCONF_4' , '20,2'],['MARGINE_4' , '20,2'],['SALDO_DISP_4' , '20,2'],['UTILIZZO_4' , '20,2'],['FIDO_5' , '20,2'],['SALDO_CONT_5' , '20,2'],['SCONF_5' , '20,2'],['MARGINE_5' , '20,2'],['SALDO_DISP_5' , '20,2'],['UTILIZZO_5' , '20,2'],['FIDO_6' , '20,2'],['SALDO_CONT_6' , '20,2'],['SCONF_6' , '20,2'],['MARGINE_6' , '20,2'],['SALDO_DISP_6' , '20,2'],['UTILIZZO_6' , '20,2'],['FIDO_7' , '20,2'],['SALDO_CONT_7' , '20,2'],['SCONF_7' , '20,2'],['MARGINE_7' , '20,2'],['SALDO_DISP_7' , '20,2'],['UTILIZZO_7' , '20,2'],['FIDO_8' , '20,2'],['SALDO_CONT_8' , '20,2'],['SCONF_8' , '20,2'],['MARGINE_8' , '20,2'],['SALDO_DISP_8' , '20,2'],['UTILIZZO_8' , '20,2'],['FIDO_9' , '20,2'],['SALDO_CONT_9' , '20,2'],['SCONF_9' , '20,2'],['MARGINE_9' , '20,2'],['SALDO_DISP_9' , '20,2'],['UTILIZZO_9' , '20,2'],['FIDO_10' , '20,2'],['SALDO_CONT_10' , '20,2'],['SCONF_10' , '20,2'],['MARGINE_10' , '20,2'],['SALDO_DISP_10' , '20,2'],['UTILIZZO_10' , '20,2'],['FIDO_11' , '20,2'],['SALDO_CONT_11' , '20,2'],['SCONF_11' , '20,2'],['MARGINE_11' , '20,2'],['SALDO_DISP_11' , '20,2'],['UTILIZZO_11' , '20,2'],['FIDO_12' , '20,2'],['SALDO_CONT_12' , '20,2'],['SCONF_12' , '20,2'],['MARGINE_12' , '20,2'],['SALDO_DISP_12' , '20,2'],['UTILIZZO_12' , '20,2'],['FIDO_13' , '20,2'],['SALDO_CONT_13' , '20,2'],['SCONF_13' , '20,2'],['MARGINE_13' , '20,2'],['SALDO_DISP_13' , '20,2'],['UTILIZZO_13' , '20,2'],['FIDO_14' , '20,2'],['SALDO_CONT_14' , '20,2'],['SCONF_14' , '20,2'],['MARGINE_14' , '20,2'],['SALDO_DISP_14' , '20,2'],['UTILIZZO_14' , '20,2'],['FIDO_15' , '20,2'],['SALDO_CONT_15' , '20,2'],['SCONF_15' , '20,2'],['MARGINE_15' , '20,2'],['SALDO_DISP_15' , '20,2'],['UTILIZZO_15' , '20,2'],['FIDO_16' , '20,2'],['SALDO_CONT_16' , '20,2'],['SCONF_16' , '20,2'],['MARGINE_16' , '20,2'],['SALDO_DISP_16' , '20,2'],['UTILIZZO_16' , '20,2'],['FIDO_17' , '20,2'],['SALDO_CONT_17' , '20,2'],['SCONF_17' , '20,2'],['MARGINE_17' , '20,2'],['SALDO_DISP_17' , '20,2'],['UTILIZZO_17' , '20,2'],['FIDO_18' , '20,2'],['SALDO_CONT_18' , '20,2'],['SCONF_18' , '20,2'],['MARGINE_18' , '20,2'],['SALDO_DISP_18' , '20,2'],['UTILIZZO_18' , '20,2'],['FIDO_19' , '20,2'],['SALDO_CONT_19' , '20,2'],['SCONF_19' , '20,2'],['MARGINE_19' , '20,2'],['SALDO_DISP_19' , '20,2'],['UTILIZZO_19' , '20,2'],['FIDO_20' , '20,2'],['SALDO_CONT_20' , '20,2'],['SCONF_20' , '20,2'],['MARGINE_20' , '20,2'],['SALDO_DISP_20' , '20,2'],['UTILIZZO_20' , '20,2'],['FIDO_21' , '20,2'],['SALDO_CONT_21' , '20,2'],['SCONF_21' , '20,2'],['MARGINE_21' , '20,2'],['SALDO_DISP_21' , '20,2'],['UTILIZZO_21' , '20,2'],['FIDO_22' , '20,2'],['SALDO_CONT_22' , '20,2'],['SCONF_22' , '20,2'],['MARGINE_22' , '20,2'],['SALDO_DISP_22' , '20,2'],['UTILIZZO_22' , '20,2'],['FIDO_23' , '20,2'],['SALDO_CONT_23' , '20,2'],['SCONF_23' , '20,2'],['MARGINE_23' , '20,2'],['SALDO_DISP_23' , '20,2'],['UTILIZZO_23' , '20,2'],['FIDO_24' , '20,2'],['SALDO_CONT_24' , '20,2'],['SCONF_24' , '20,2'],['MARGINE_24' , '20,2'],['SALDO_DISP_24' , '20,2'],['UTILIZZO_24' , '20,2'],['FIDO_25' , '20,2'],['SALDO_CONT_25' , '20,2'],['SCONF_25' , '20,2'],['MARGINE_25' , '20,2'],['SALDO_DISP_25' , '20,2'],['UTILIZZO_25' , '20,2'],['FIDO_26' , '20,2'],['SALDO_CONT_26' , '20,2'],['SCONF_26' , '20,2'],['MARGINE_26' , '20,2'],['SALDO_DISP_26' , '20,2'],['UTILIZZO_26' , '20,2'],['FIDO_27' , '20,2'],['SALDO_CONT_27' , '20,2'],['SCONF_27' , '20,2'],['MARGINE_27' , '20,2'],['SALDO_DISP_27' , '20,2'],['UTILIZZO_27' , '20,2'],['FIDO_28' , '20,2'],['SALDO_CONT_28' , '20,2'],['SCONF_28' , '20,2'],['MARGINE_28' , '20,2'],['SALDO_DISP_28' , '20,2'],['UTILIZZO_28' , '20,2'],['FIDO_29' , '20,2'],['SALDO_CONT_29' , '20,2'],['SCONF_29' , '20,2'],['MARGINE_29' , '20,2'],['SALDO_DISP_29' , '20,2'],['UTILIZZO_29' , '20,2'],['FIDO_30' , '20,2'],['SALDO_CONT_30' , '20,2'],['SCONF_30' , '20,2'],['MARGINE_30' , '20,2'],['SALDO_DISP_30' , '20,2'],['UTILIZZO_30' , '20,2'],['FIDO_31' , '20,2'],['SALDO_CONT_31' , '20,2'],['SCONF_31' , '20,2'],['MARGINE_31' , '20,2'],['SALDO_DISP_31' , '20,2'],['UTILIZZO_31' , '20,2'],['DT_ELABORAZIONE' , 'DATE9.']]\n",
    "\n",
    "\n",
    "sas_file_path = r\"C:\\Users\\EE01258\\OneDrive - Unicredit\\Desktop\\45cols_202510.sas7bdat\"\n",
    "sas_file_base_name = os.path.basename(sas_file_path)\n",
    "\n",
    "target_extension = \".parquet\"\n",
    "file_prefix = sas_file_base_name.replace(\".sas7bdat\",\"\") \n",
    "table_folder = file_prefix\n",
    "chunks_folder = os.path.join(\"chunks_folder\", table_folder) #for Chunked Parquets\n",
    "output_folder = os.path.join(\"output_folder\", table_folder) #for consolidated Parquets\n",
    "\n",
    "col_list = [col[0] for col in colspecs]\n",
    "\n",
    "offset_days = 3653\n",
    "offset_seconds = 315619200\n",
    "\n",
    "chunksize = 1000000\n",
    "offset = 0\n",
    "chunk_idx = 0\n",
    "max_workers = 1\n",
    "\n",
    "def timenow():\n",
    "    timenow = datetime.now().strftime('%H:%M:%S')\n",
    "    return timenow\n",
    "\n",
    "def map_sas_to_arrow(sas_format):\n",
    "\n",
    "    if sas_format.startswith(\"$\"):\n",
    "        return pa.string()\n",
    "\n",
    "    elif sas_format.upper().startswith(\"DATE\"):\n",
    "        return  pa.date32()\n",
    "\n",
    "    elif sas_format.upper().startswith(\"DATETIME\"):\n",
    "        return pa.timestamp('s')\n",
    "\n",
    "    elif \",\" in sas_format.upper():\n",
    "        precision, scale = sas_format.split(\",\")\n",
    "        return pa.decimal128(int(precision), int(scale))\n",
    "\n",
    "    elif sas_format.isdigit():\n",
    "        return pa.int64()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo SAS non gestito: {sas_format}\")\n",
    "\n",
    "def build_schema(colspecs):\n",
    "    \n",
    "    # We need two schema:\n",
    "    # the first one when we convert to Arrow from dictionaries\n",
    "    # the second when we fix date/datetime columns (we do it later because is more performant than doing it during SAS Conversion)\n",
    "\n",
    "    fields_bc = []\n",
    "    fields_ac = []\n",
    "    date_fields = []\n",
    "    datetime_fields = []\n",
    "\n",
    "    for column in colspecs:\n",
    "        col_name,sas_format = column[0], column[1]\n",
    "        col_type = map_sas_to_arrow(sas_format)\n",
    "        \n",
    "        if col_type == pa.date32():\n",
    "\n",
    "            col_type_bc = pa.int64()\n",
    "            col_type_ac = pa.date32()\n",
    "            date_fields.append(col_name)\n",
    "\n",
    "        if col_type == pa.timestamp('s'):\n",
    "\n",
    "            col_type_bc = pa.int64()\n",
    "            col_type_ac = pa.timestamp('s')\n",
    "            datetime_fields.append(col_name)\n",
    "        \n",
    "        else: \n",
    "            col_type_bc = col_type\n",
    "            col_type_ac = col_type\n",
    "\n",
    "        fields_bc.append(pa.field(col_name,col_type_bc))\n",
    "        fields_ac.append(pa.field(col_name,col_type_ac))\n",
    "\n",
    "    arrow_schema_before_cast = pa.schema(fields_bc)\n",
    "    arrow_schema_after_cast  = pa.schema(fields_ac)\n",
    "\n",
    "    return arrow_schema_before_cast,arrow_schema_after_cast, date_fields, datetime_fields\n",
    "\n",
    "def file_checker(chunks_folder, sas_file_path, chunksize):\n",
    "         \n",
    "    if os.path.exists(chunks_folder):\n",
    "            shutil.rmtree(chunks_folder)\n",
    "    os.mkdir(chunks_folder)\n",
    "\n",
    "    #Controllo_Iniziale_File\n",
    "\n",
    "    if os.path.exists(sas_file_path):\n",
    "        _,meta = pyreadstat.read_sas7bdat(sas_file_path, metadataonly = True)\n",
    "    else:\n",
    "        print(f\"File_Check: File {sas_file_path} non trovato\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"A1\")\n",
    "    rows = meta.number_rows\n",
    "    num_chunks = (rows + chunksize - 1)//chunksize\n",
    "    del _\n",
    "    del meta\n",
    "\n",
    "    print(f\"File Trovato: #Rows: {rows}, chunksize: {chunksize}, #Chunks:{num_chunks}\")\n",
    "    return chunksize,num_chunks \n",
    "\n",
    "\n",
    "#Operator\n",
    "def process_chunk(chunks_folder, file_prefix, chunk_idx, chunksize, arrow_schema_before_cast, arrow_schema_after_cast, date_fields, datetime_fields):\n",
    "\n",
    "    if chunk_idx == 0: print(\"Chunk Processing: Start\")\n",
    "    filepath = os.path.join(chunks_folder, file_prefix + \"#\"  + str(chunk_idx) + \".parquet\")\n",
    "\n",
    "    chunk_start = time.time()\n",
    "    offset = chunk_idx * chunksize\n",
    "    \n",
    "    try:\n",
    "        chunk, _ = pyreadstat.read_sas7bdat(\n",
    "            sas_file_path,\n",
    "            row_offset= offset,\n",
    "            row_limit= chunksize,\n",
    "            output_format = \"dict\",\n",
    "            encoding = \"windows-1252\",\n",
    "            disable_datetime_conversion = True,\n",
    "            usecols = col_list\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"{chunk_idx}: Riga{chunk_idx + 1}: Errore critico in lettura: {e}\")\n",
    "        sys.exit(1)\n",
    "    gc.collect()\n",
    "\n",
    "    #create arrow table and delete dictionaries to free RAM\n",
    "    try:\n",
    "        table = pa.table(chunk, schema = arrow_schema_before_cast)\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion_to_Arrow: ERROR --> {e}\")\n",
    "        sys.exit(1)\n",
    "    del chunk, _\n",
    "\n",
    "    #offset Date and datetime columns, then cast\n",
    "    casted_columns = []\n",
    "    for field,col in zip(table.schema,table.columns):\n",
    "\n",
    "        if field.name in date_fields:  \n",
    "            adjusted_col = pc.subtract(col, pa.scalar(offset_days))\n",
    "            casted_col = pc.cast(adjusted_col, pa.date32())\n",
    "            casted_columns.append(casted_col)\n",
    "\n",
    "        elif field.name in datetime_fields:\n",
    "            adjusted_col = pc.subtract(col, pa.scalar(offset_seconds))\n",
    "            casted_col = pc.cast(adjusted_col, pa.timestamp('s'))\n",
    "            casted_columns.append(casted_col)       \n",
    "\n",
    "        else: casted_columns.append(col)     \n",
    "    \n",
    "    casted_table = pa.Table.from_arrays (casted_columns, schema =  arrow_schema_after_cast)\n",
    "    del table\n",
    "\n",
    "    pq.write_table(casted_table, filepath, compression = 'zstd', compression_level = 1)\n",
    "    del casted_table\n",
    "\n",
    "    gc.collect()\n",
    "    chunk_end = time.time()\n",
    "    chunk_elab_time = round((chunk_end - chunk_start),0)\n",
    "\n",
    "    print(f\"Chunk {chunk_idx}: Completed ‚úÖ | ElabTime: {chunk_elab_time} s | {timenow()}\")\n",
    "\n",
    " \n",
    "def consolidate_parquet(chunks_folder, output_folder, file_prefix):\n",
    "    \n",
    "    print(\"Consolidazione Parquet Finale\")\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    consolidation_start = time.time()\n",
    "\n",
    "    files = sorted(glob.glob(os.path.join(chunks_folder, \"*.parquet\")),\n",
    "                   key=lambda f: int(f.split(\"#\")[-1].split(\".\")[0]))\n",
    "\n",
    "    out_path = os.path.join(output_folder, f\"{file_prefix}.parquet\")\n",
    "    writer = None\n",
    "    try:\n",
    "        for file in files:\n",
    "            table = pq.read_table(file)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, table.schema, compression=\"zstd\", compression_level=3)\n",
    "            writer.write_table(table)\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "#Main with Iteration\n",
    "def main():\n",
    "\n",
    "    chunksize, num_chunks = file_checker(\n",
    "        chunks_folder = chunks_folder,\n",
    "        sas_file_path = sas_file_path,\n",
    "        chunksize = chunksize\n",
    "    )\n",
    "\n",
    "    arrow_schema_before_cast, arrow_schema_after_cast, date_fields, datetime_fields = build_schema(colspecs = colspecs)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    process_chunk,\n",
    "                    chunks_folder,\n",
    "                    file_prefix, \n",
    "                    chunk_idx,\n",
    "                    chunksize,\n",
    "                    arrow_schema_before_cast,\n",
    "                    arrow_schema_after_cast,\n",
    "                    date_fields,\n",
    "                    datetime_fields\n",
    "                    ): chunk_idx for chunk_idx in range(num_chunks)\n",
    "            }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        idx = futures[future]\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"[Chunk {idx}] Producer error: {e}\")\n",
    "\n",
    "    consolidate_parquet(\n",
    "         chunks_folder = chunks_folder,\n",
    "         output_folder = output_folder,\n",
    "         file_prefix = file_prefix\n",
    "    )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Schema del dataset:\n",
      "\n",
      "üìä Numero di righe: 8560000\n",
      "üì¶ Numero di colonne: 15\n",
      "\n",
      "‚è±Ô∏è Tempo di lettura: 0.861 secondi\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "input_folder = \"output_folder_zstd\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Leggi tutti i file Parquet nella cartella come dataset\n",
    "dataset = ds.dataset(input_folder, format=\"parquet\")\n",
    "\n",
    "# Carica l'intero dataset in memoria (solo per test)\n",
    "table = dataset.to_table()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "# Stampa informazioni\n",
    "print(\"üìÑ Schema del dataset:\")\n",
    "print(f\"\\nüìä Numero di righe: {table.num_rows}\")\n",
    "print(f\"üì¶ Numero di colonne: {table.num_columns}\")\n",
    "print(\"\\n‚è±Ô∏è Tempo di lettura: {:.3f} secondi\".format(elapsed))\n",
    "\n",
    "del table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
